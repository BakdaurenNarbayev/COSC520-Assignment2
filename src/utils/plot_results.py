"""
This script visualizes benchmarking results for Range Minimum Query (RMQ)
data structures such as Naive, SRD, SegmentTree, and SparseTable.

It generates:
    • Log-log plots for preprocessing, query, and update times.
    • Memory usage plots (in MB).
    • Exported figures in PDF format.

Expected Input:
    - A CSV file `benchmark_results.csv` generated by `benchmark.py`

Output:
    - PDF plots saved under `../results/plots/`
"""

import matplotlib.pyplot as plt
import csv
from collections import defaultdict
import numpy as np
import os

RESULTS_FILE = "..\\results\\benchmark_results.csv"
OUTPUT_DIR = "..\\results\\plots"

os.makedirs(OUTPUT_DIR, exist_ok=True)

def load_results():
    """
    Load benchmark results from the CSV file into a nested dictionary structure.

    The returned dictionary is organized as:
        results[metric][N][algorithm] = {
            "mean": float,
            "std": float,
            "mem": float | None
        }

    Returns:
        dict: Nested results structure containing all parsed benchmark data.
    """
    results = defaultdict(lambda: defaultdict(dict))
    with open(RESULTS_FILE, "r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            metric = row["Metric"]
            n = int(row["N"])
            algo = row["Algorithm"]
            mean = float(row["Mean"])
            std = float(row["StdDev"])
            mem = row.get("Memory_MB")
            mem_val = float(mem) if mem else None
            results[metric][n][algo] = {"mean": mean, "std": std, "mem": mem_val}
    return results

def plot_metric(results, ylabel, filename):
    """
    Generate a log-log performance plot for a specific benchmark metric.

    Args:
        results (dict): Data loaded via `load_results()`.
        ylabel (str): The name of the metric (e.g., "build", "query").
        filename (str): Output filename.

    Output:
        Saves a PDF figure to OUTPUT_DIR/<filename>.pdf
    """
    sizes = sorted(results.keys())
    all_algorithms = set()
    for n in sizes:
        all_algorithms.update(results[n].keys())

    plt.figure(figsize=(8, 6))
    for algo in sorted(all_algorithms):
        Ns, means, stds = [], [], []
        for n in sizes:
            entry = results[n].get(algo)
            if entry is None:
                continue
            Ns.append(n)
            means.append(entry["mean"])
            stds.append(entry["std"])
        if not Ns:
            continue
        Ns = np.array(Ns)
        means = np.array(means)
        stds = np.array(stds)
        plt.plot(Ns, means, marker='o', label=algo)
        plt.fill_between(Ns, means - stds, means + stds, alpha=0.2)

    plt.title(f"{ylabel} vs Array Size (log-log scale)")
    plt.xlabel("Array Size (N)")
    plt.ylabel(ylabel)
    plt.xscale("log")
    plt.yscale("log")
    plt.legend()
    plt.grid(visible=False)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, f"{filename}.pdf"))
    plt.close()

def plot_memory_usage(results, filename="memory_usage"):
    """
    Generate a log-log plot of memory usage vs dataset size.

    Args:
        results (dict): Data from `load_results()["build"]`.
        filename (str, optional): Output filename.
    """
    sizes = sorted(results.keys())
    all_algorithms = set()
    for n in sizes:
        all_algorithms.update(results[n].keys())

    plt.figure(figsize=(8, 6))
    for algo in sorted(all_algorithms):
        Ns, mems = [], []
        for n in sizes:
            entry = results[n].get(algo)
            if entry is None or entry["mem"] is None:
                continue
            Ns.append(n)
            mems.append(entry["mem"])
        if not Ns:
            continue
        Ns = np.array(Ns)
        mems = np.array(mems)
        plt.plot(Ns, mems, marker='o', label=algo)

    plt.title("Memory Usage vs Array Size (log-log scale)")
    plt.xlabel("Array Size (N)")
    plt.ylabel("Memory Usage (MB)")
    plt.xscale("log")
    plt.yscale("log")
    plt.legend()
    plt.grid(visible=False)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, f"{filename}.pdf"))
    plt.close()

if __name__ == "__main__":
    results = load_results()
    metrics_to_plot = [
        ("build", "Preprocessing Time (s)"),
        ("query", "Query Time (s)"),
        ("update", "Update Time (s)")
    ]
    for metric, ylabel in metrics_to_plot:
        plot_metric(results[metric], ylabel, metric)
    plot_memory_usage(results["build"], filename="memory_usage")
    print(f"All plots saved in {OUTPUT_DIR}")
